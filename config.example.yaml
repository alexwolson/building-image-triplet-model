# Example configuration file for building-image-triplet-model
# Copy this file to config.yaml and update the paths for your environment

auto_batch_size:
  enabled: false  # Set to true to automatically find the best batch size
  mode: power     # Options: power, binsearch

data:
  # Path to the processed HDF5 (Hierarchical Data Format) dataset file
  # Update this path to point to your dataset location
  hdf5_path: "data/processed/dataset.h5"
  
  # Input directory containing raw images
  # Update this path to point to your raw image directory
  input_dir: "data/raw/images"
  
  # Training parameters
  batch_size: 32                  # Number of triplets per batch
  num_workers: 4                  # Number of data loading workers
  num_difficulty_levels: 5        # Number of distance-based difficulty buckets for triplet sampling
  ucb_alpha: 2.0                  # UCB (Upper Confidence Bound) exploration parameter for difficulty selection
  cache_size: 1000                # Number of embeddings to cache in memory
  
  # Model configuration for preprocessing
  feature_model: vit_pe_spatial_base_patch16_512.fb  # Backbone model for feature extraction
  image_size: 512                 # Input image size for the backbone model
  
  # Multi-GPU preprocessing settings
  devices: "auto"                 # Number of GPUs to use for preprocessing ("auto", 1, 2, etc.)
  accelerator: "auto"             # Device type ("auto", "cuda", "cpu")
  strategy: "auto"                # Multi-GPU strategy ("auto", "ddp", "ddp_spawn")

logging:
  project_name: "geo-triplet-net"
  exp_name: null  # Set to a specific name for this experiment
  checkpoint_dir: "checkpoints"
  offline: false  # Set to true to disable wandb logging

model:
  backbone: vit_pe_spatial_base_patch16_512.fb
  embedding_size: 128
  margin: 1.0
  # Backbone output size for precomputed embeddings (auto-determined if not set)
  # For vit_pe_spatial_base_patch16_512.fb, this is typically 768
  backbone_output_size: null

train:
  difficulty_update_freq: 100     # Update difficulty statistics every N batches
  lr: 0.0001                      # Learning rate for optimizer (AdamW)
  max_epochs: 100                 # Maximum number of training epochs
  precision: 16-mixed             # Mixed precision training (16-bit for speed, 32-bit for stability)
  samples_per_epoch: 5000         # Number of triplet samples per epoch
  seed: 42                        # Random seed for reproducibility
  warmup_epochs: 3                # Number of epochs for learning rate warmup
  weight_decay: 0.0001            # L2 regularization weight decay
